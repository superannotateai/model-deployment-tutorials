{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/superannotateai/model-deployment-tutorials/blob/main/JETSON/SuperAnnotate_JETSON_Deeplabv3%2B_Deployment.ipynb\"><img src=\"https://user-images.githubusercontent.com/25985824/104791629-6e618700-5769-11eb-857f-d176b37d2496.png\" height=\"32\" width=\"32\"> Try in Google Colab</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upVdTuM6i-FI"
   },
   "source": [
    "# 1. Install Prerequisites \n",
    "(Please click on **RESTART RUNTIME** button when it appears in the output of this code block)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daIHTFTus6sA"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY43J8a1lG8b"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz /content/\n",
    "!cp /content/drive/MyDrive/cudnn-11.2-linux-x64-v8.1.1.33.tgz /content/\n",
    "\n",
    "!tar -xzvf cudnn-11.2-linux-x64-v8.1.1.33.tgz\n",
    "\n",
    "!sudo cp cuda/include/cudnn*.h /usr/local/cuda/include \n",
    "!sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64 \n",
    "!sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\n",
    "\n",
    "!tar xzvf TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz\n",
    "os.environ['LD_LIBRARY_PATH'] += ':/content/TensorRT-7.2.3.4/lib/'\n",
    "\n",
    "%cd /content/TensorRT-7.2.3.4/python/\n",
    "!sudo pip3 install tensorrt-7.2.3.4-cp37-none-linux_x86_64.whl\n",
    "\n",
    "%cd /content/TensorRT-7.2.3.4/uff/\n",
    "!sudo pip3 install uff-0.6.9-py2.py3-none-any.whl\n",
    "\n",
    "%cd /content/TensorRT-7.2.3.4/graphsurgeon/\n",
    "!sudo pip3 install graphsurgeon-0.4.5-py2.py3-none-any.whl\n",
    "%cd /content/TensorRT-7.2.3.4/onnx_graphsurgeon\n",
    "!sudo pip3 install onnx_graphsurgeon-0.2.6-py2.py3-none-any.whl\n",
    "%cd /content/\n",
    "\n",
    "!pip install tensorflow-gpu==1.15.3\n",
    "!pip install tf_slim==1.0.0\n",
    "\n",
    "!git clone https://github.com/nolanliou/mobile-deeplab-v3-plus\n",
    "!pip install superannotate\n",
    "!pip install google-resumable-media==0.5.0\n",
    "\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran\n",
    "!sudo apt-get install python3-pip\n",
    "!pip3 install -U pip testresources setuptools==49.6.0\n",
    "!pip3 install future==0.18.2 mock==3.0.5 h5py==2.10.0 keras_preprocessing==1.1.1 keras_applications==1.0.8 gast==0.2.2 futures protobuf pybind11\n",
    "!pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 'tensorflow<2'\n",
    "!pip3 install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NDukI5gjVoY"
   },
   "source": [
    "# 2. Setup your SuperAnnotate token, project names of trianing images and desired output model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhvXpjlY-G3e"
   },
   "outputs": [],
   "source": [
    "TOKEN = \"For Pro Users, the Token-ID can be generated in the teams section\"\n",
    "PROJECT_NAME = \"City\"\n",
    "OUTPUT_MODEL = \"sa-tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X6NHoQdjbTf"
   },
   "source": [
    "# 3. Download and preprocess the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPKE00by-IGE"
   },
   "outputs": [],
   "source": [
    "import superannotate as sa\n",
    "import json\n",
    "import os \n",
    "from shutil import copy, make_archive\n",
    "import re\n",
    "\n",
    "os.environ['CUDA_INSTALL_DIR']= \"/usr/local/cuda/\"\n",
    "os.environ['CUDNN_INSTALL_DIR']= \"/usr/local/cuda/\"\n",
    "os.environ['LD_LIBRARY_PATH']+= \":/usr/lib64-nvidia/:/content/TensorRT-7.2.3.4/include/:/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/:/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/bin/\"\n",
    "os.environ['PATH']+= \":/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib:/content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/bin/\"\n",
    "!sudo rm /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/libnvinfer.so.7\n",
    "!sudo ln -s /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/libnvinfer.so.7.2.3 /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/libnvinfer.so.7\n",
    "!sudo cp -r /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/lib/*.* /usr/lib/ \n",
    "!sudo cp -r /content/TensorRT-7.2.3.4/targets/x86_64-linux-gnu/bin/*.* /usr/bin/ \n",
    "\n",
    "proj_folder = dict(zip(PROJECT_NAME, [\"proj_\" + str(i) for i in range(len(PROJECT_NAME))]))\n",
    "token_json = {\"token\": TOKEN}\n",
    "with open('sa_config.json', 'w') as f:\n",
    "  json.dump(token_json, f)\n",
    "\n",
    "sa.init('sa_config.json')\n",
    "\n",
    "dataset_base_dir = '/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg'\n",
    "if not os.path.exists(dataset_base_dir):\n",
    "  os.mkdir(dataset_base_dir)\n",
    "\n",
    "export = sa.prepare_export(PROJECT_NAME, annotation_statuses=[\"Completed\"], include_fuse=True)\n",
    "sa.download_export(PROJECT_NAME, export, dataset_base_dir)\n",
    "\n",
    "if not os.path.exists(dataset_base_dir + '/VOC2012'):\n",
    "  os.makedirs(dataset_base_dir + '/VOC2012/JPEGImages')\n",
    "  os.makedirs(dataset_base_dir + '/VOC2012/SegmentationClass')\n",
    "  os.makedirs(dataset_base_dir + '/VOC2012/ImageSets/Segmentation')\n",
    "  os.makedirs(dataset_base_dir + '/tfrecord')\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/\n",
    "!rm -f *.json\n",
    "!rm -f *___save.png\n",
    "\n",
    "folder_content = os.listdir('.')\n",
    "for entry in folder_content:\n",
    "  if len(entry.split('.'))==1:\n",
    "    continue\n",
    "  if entry.endswith('___fuse.png'):\n",
    "    im_name, _ = os.path.splitext(entry[:-11])\n",
    "    copy(entry, 'VOC2012/SegmentationClass/' + im_name + '.png')\n",
    "  else:\n",
    "    copy(entry, 'VOC2012/JPEGImages/' + entry)\n",
    "\n",
    "with open(dataset_base_dir + '/VOC2012/ImageSets/Segmentation/trainval.txt', 'w') as f:\n",
    "  image_names = os.listdir(dataset_base_dir + '/VOC2012/JPEGImages')\n",
    "  for image_name in image_names:\n",
    "    file_name, _ = os.path.splitext(image_name)\n",
    "    f.write(file_name + '\\n')\n",
    "  \n",
    "data_size = len(image_names)\n",
    "val_size = int(data_size * 0.1)\n",
    "train_size = data_size - val_size\n",
    "!shuf VOC2012/ImageSets/Segmentation/trainval.txt | tee >(head -n \"{val_size}\" > VOC2012/ImageSets/Segmentation/val.txt) | tail -n \"{train_size}\" > VOC2012/ImageSets/Segmentation/train.txt\n",
    "!python ../remove_gt_colormap.py --original_gt_folder=\"VOC2012/SegmentationClass\" --output_dir=\"VOC2012/SegmentationClassRaw\"\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/datasets/\n",
    "\n",
    "with open('build_voc2012_data.py') as f:\n",
    "  record_gen = f.read()\n",
    "\n",
    "record_gen = re.sub(r\"import pdb\\; pdb\\.set\\_trace\\(\\)\\n  \", \"\", record_gen)\n",
    "\n",
    "with open('build_voc2012_data.py', 'w') as f:\n",
    "  f.write(record_gen)\n",
    "\n",
    "!python build_voc2012_data.py \\\n",
    "  --image_folder=\"pascal_voc_seg/VOC2012/JPEGImages\" \\\n",
    "  --semantic_segmentation_folder=\"pascal_voc_seg/VOC2012/SegmentationClassRaw\" \\\n",
    "  --list_folder=\"pascal_voc_seg/VOC2012/ImageSets/Segmentation\" \\\n",
    "  --image_format=\"jpg\" \\\n",
    "  --output_dir=\"pascal_voc_seg/tfrecord/\"\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/\n",
    "objdata = 'segmentation_dataset.py'\n",
    "with open(objdata) as f:\n",
    "    s = f.read()\n",
    "\n",
    "with open('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/classes/classes.json', 'r') as f:\n",
    "  class_data = json.load(f)\n",
    "\n",
    "num_classes = len(class_data) + 1\n",
    "s = re.sub('num_classes=21','num_classes=' + str(num_classes),s)\n",
    "s = re.sub(\"'train': 1464\",\"'train': \" + str(train_size),s)\n",
    "s = re.sub(\"'trainval': 2913\",\"'trainval': \" + str(data_size),s)\n",
    "s = re.sub(\"'val': 1449\",\"'val': \" + str(val_size),s)\n",
    "\n",
    "with open(objdata, 'w') as f:\n",
    "  f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKcwpEHBjpj9"
   },
   "source": [
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZAIf5rboLnr"
   },
   "outputs": [],
   "source": [
    "%env PYTHONPATH=/content/mobile-deeplab-v3-plus:/env/python\n",
    "\n",
    "import tensorflow as tf\n",
    "from mobilenet_v2 import MobilenetV2\n",
    "import urllib\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/\n",
    "if not os.path.exists('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2'):\n",
    "  os.mkdir('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2')\n",
    "\n",
    "%cd mnv2 \n",
    "!wget https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz\n",
    "!tar xvf mobilenet_v2_1.0_224.tgz\n",
    "\n",
    "if not os.path.exists('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted'):\n",
    "  os.mkdir('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted')\n",
    "\n",
    "%cd /content/mobile-deeplab-v3-plus/\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# For simplicity we just decode jpeg inside tensorflow.\n",
    "# But one can provide any input obviously.\n",
    "file_input = tf.placeholder(tf.string, ())\n",
    "\n",
    "image = tf.image.decode_jpeg(tf.read_file(file_input))\n",
    "\n",
    "images = tf.expand_dims(image, 0)\n",
    "images = tf.cast(images, tf.float32) / 128. - 1\n",
    "images.set_shape((None, None, None, 3))\n",
    "images = tf.image.resize_images(images, (224, 224))\n",
    "mobilenet_model = MobilenetV2()\n",
    "\n",
    "logits, endpoints = mobilenet_model.forward(images, is_training=False)\n",
    "\n",
    "exclude = ['global_step']\n",
    "variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "variables_map = {}\n",
    "for v in variables_to_restore:\n",
    "  old_name = v.name.split(':')[0]\n",
    "  old_name = old_name.replace('conv2d/kernel', 'weights')\n",
    "  old_name = old_name.replace('conv2d/bias', 'biases')\n",
    "  variables_map[old_name] = v\n",
    "tf.train.init_from_checkpoint('/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2/mobilenet_v2_1.0_224.ckpt', variables_map)\n",
    "filename, _ = urllib.request.urlretrieve('https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  x = endpoints['Predictions'].eval(feed_dict={file_input: filename})\n",
    "  saver.save(sess, '/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted/mobilenet-v2-224.ckpt')\n",
    "\n",
    "if not os.path.exists(\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\"):\n",
    "  os.mkdir(\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\")\n",
    "\n",
    "train_iters = 15000\n",
    "%cd /content/mobile-deeplab-v3-plus/\n",
    "!python run.py --dataset_dir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/tfrecord\"\\\n",
    "  --dataset_name=\"pascal_voc2012\"\\\n",
    "  --logdir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\"\\\n",
    "  --model_type=\"deeplab-v3-plus\"\\\n",
    "  --train_subset=\"train\"\\\n",
    "  --base_learning_rate=0.001\\\n",
    "  --num_clones=1\\\n",
    "  --model_input_size=256\\\n",
    "  --model_input_size=256\\\n",
    "  --training_number_of_steps=\"{train_iters}\"\\\n",
    "  --pretrained_backbone_model_dir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/mnv2_adapted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK96y2i6jrp9"
   },
   "source": [
    "# 5. Generate and download the files needed for the OAK-D device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyYgxtY0LC9d"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.colab import files\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "if not os.path.exists('/content/export'):\n",
    "  os.mkdir('/content/export')\n",
    "\n",
    "!python run.py --dataset_dir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/tfrecord\"\\\n",
    "  --dataset_name=\"pascal_voc2012\"\\\n",
    "  --logdir=\"/content/mobile-deeplab-v3-plus/datasets/pascal_voc_seg/checkpoint\"\\\n",
    "  --model_type=\"deeplab-v3-plus\"\\\n",
    "  --model_input_size=256\\\n",
    "  --model_input_size=256\\\n",
    "  --mode=export\\\n",
    "  --export_dir=\"/content/export\"\n",
    "\n",
    "save_model_dir = \"/content/export/\" + os.listdir(\"/content/export\")[0]\n",
    "\n",
    "converter = trt.TrtGraphConverter(input_saved_model_dir=\"/content/export/1623136798\", precision_mode=\"FP16\")\n",
    "converter.convert()\n",
    "converter.save(\"/content/trt\")\n",
    "\n",
    "make_archive(OUTPUT_MODEL, 'zip', \"/content/trt\")\n",
    "files.download(OUTPUT_MODEL + '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUum5lRhS60d"
   },
   "source": [
    "# 6. Test the custom model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4pbRWP3S-Cz"
   },
   "source": [
    "Afterwards you can load the files in the archive with tf-trt on jetson devices and integrate it to your pipelines."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qUum5lRhS60d",
    "0fbqBDdyU1Da"
   ],
   "name": "SuperAnnotate/Jetson: Deeplabv3+ Deployment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
